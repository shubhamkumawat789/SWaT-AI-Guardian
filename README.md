# ğŸ›¡ï¸ Secure Water Treatment (SWaT) Anomaly Detection System

## ğŸ¯ Project Overview

An advanced real-time anomaly detection system for Secure Water Treatment (SWaT) infrastructure using deep learning, streaming analytics, and distributed computing.

## ğŸ§° Tech Stack

### **Programming & Data Processing**
- **Python 3.9+** - Core programming language
- **NumPy** - Numerical computing
- **Pandas** - Data manipulation and analysis

### **Machine Learning & Deep Learning**
- **TensorFlow/Keras** - Autoencoder model for anomaly detection
- **Scikit-learn** - Preprocessing, scaling, Isolation Forest (optional ensemble)

### **Streaming & Big Data**
- **Apache Kafka** - Real-time data streaming and message queuing
- **Apache Spark** - Distributed data processing
- **Spark Structured Streaming** - Stream processing with event-time handling
- **PySpark SQL** - Feature engineering and window functions

### **Backend & API**
- **FastAPI** - High-performance REST API
- **Uvicorn** - ASGI server for FastAPI

### **Data Ingestion**
- **Kafka-Python** - Python client for Kafka

### **Visualization & Dashboard**
- **Streamlit** - Interactive web dashboard
- **Altair** - Declarative statistical visualization
- **Matplotlib** - Static plotting
- **Seaborn** - Statistical data visualization
- **Plotly** - Interactive charts

### **Dataset**
- **SWaT (Secure Water Treatment)** - Industrial control system dataset

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SWaT Sensors   â”‚
â”‚  (CSV Simulation)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer  â”‚ â—„â”€â”€ Streams sensor data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topics   â”‚
â”‚  - sensor-data  â”‚
â”‚  - anomalies    â”‚
â”‚  - alerts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Consumer  â”‚
â”‚   + Spark       â”‚ â—„â”€â”€ Window-based feature engineering
â”‚  Streaming      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Inference Engine       â”‚
â”‚  - Autoencoder (TensorFlow) â”‚
â”‚  - Isolation Forest (sklearn)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI     â”‚ â”‚ Alerting â”‚ â”‚  Streamlit  â”‚
â”‚  REST API    â”‚ â”‚  System  â”‚ â”‚  Dashboard  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
secure-water-treatment/
â”œâ”€â”€ .agent/
â”‚   â””â”€â”€ workflows/          # Development workflows
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.yaml   # Kafka configuration
â”‚   â”œâ”€â”€ spark_config.yaml   # Spark settings
â”‚   â””â”€â”€ model_config.yaml   # Model hyperparameters
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original SWaT dataset
â”‚   â”œâ”€â”€ processed/          # Preprocessed data
â”‚   â””â”€â”€ streaming/          # Streaming data buffer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ autoencoder/        # Trained autoencoder models
â”‚   â”‚   â”œâ”€â”€ model.h5
â”‚   â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”‚   â””â”€â”€ threshold.json
â”‚   â””â”€â”€ isolation_forest/   # Isolation Forest models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py    # Stream data to Kafka
â”‚   â”‚   â””â”€â”€ kafka_consumer.py    # Consume from Kafka
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ spark_preprocessing.py  # Spark-based preprocessing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train_autoencoder.py    # Train deep learning model
â”‚   â”‚   â””â”€â”€ train_isolation_forest.py  # Train ensemble model
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ inference_engine.py     # Core inference logic
â”‚   â”‚   â””â”€â”€ streaming_inference.py  # Real-time inference
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ fastapi_server.py       # REST API endpoints
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ streamlit_app.py        # Interactive dashboard
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb  # Data exploration
â”œâ”€â”€ tests/                  # Unit and integration tests
â”œâ”€â”€ logs/                   # Application logs
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ docker-compose.yml      # Multi-container orchestration
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ README.md              # This file
```

---

## ğŸš€ Implementation Roadmap

### **Phase 1: Project Setup** (Week 1) âœ…
- [x] Create project structure
- [x] Set up virtual environment
- [x] Install dependencies
- [x] Configuration management

### **Phase 2: Kafka Integration** (Week 2)
- [ ] Install and configure Kafka
- [ ] Create Kafka producer (sensor data simulation)
- [ ] Create Kafka consumer
- [ ] Set up topics (sensor-data, anomalies, alerts)

### **Phase 3: Spark Integration** (Week 3)
- [ ] Set up PySpark environment
- [ ] Implement Spark Structured Streaming
- [ ] Migrate feature engineering to PySpark
- [ ] Window-based aggregations with event-time processing

### **Phase 4: Enhanced ML Models** (Week 4)
- [ ] Train Isolation Forest for ensemble detection
- [ ] Implement model versioning
- [ ] Create ensemble inference pipeline
- [ ] Optimize model performance

### **Phase 5: FastAPI Backend** (Week 5)
- [ ] Create REST API with FastAPI
- [ ] Implement prediction endpoints
- [ ] Add WebSocket for real-time updates
- [ ] Authentication and rate limiting

### **Phase 6: Advanced Alerting** (Week 6)
- [ ] Multi-level alerting (INFO, WARNING, CRITICAL)
- [ ] Structured logging with JSON format
- [ ] Alert aggregation and deduplication
- [ ] Monitoring dashboard

### **Phase 7: Visualization** (Week 7)
- [ ] Enhanced Streamlit dashboard
- [ ] Real-time Kafka stream visualization
- [ ] Automated report generation
- [ ] Export capabilities (PDF, CSV)

### **Phase 8: Testing & Optimization** (Week 8)
- [ ] Unit tests for all components
- [ ] Integration tests (end-to-end)
- [ ] Performance optimization
- [ ] Comprehensive documentation

### **Phase 9: Deployment** (Week 9-10)
- [ ] Docker containerization
- [ ] Docker Compose orchestration
- [ ] CI/CD pipeline
- [ ] Production deployment

---

## ğŸ› ï¸ Quick Start

### **1. Clone and Setup**
```bash
cd "C:\Users\Shubham\Documents\Secure Water Treatment System"
python -m venv venv
.\venv\Scripts\activate
pip install -r requirements.txt
```

### **2. Train the Model**
```bash
python src/models/train_autoencoder.py
```

### **3. Run Streaming Inference**
```bash
python src/inference/streaming_inference.py
```

### **4. Launch Dashboard**
```bash
streamlit run src/dashboard/streamlit_app.py
```

### **5. Start FastAPI Server** (Coming in Phase 5)
```bash
uvicorn src.api.fastapi_server:app --reload
```

---

## ğŸ“Š Model Details

### **Autoencoder Architecture**
- **Input Layer**: Window features (mean, std, min, max, slope)
- **Encoder**: Dense layers with ReLU activation
- **Bottleneck**: Compressed representation
- **Decoder**: Reconstruction layers
- **Output**: Reconstructed features
- **Loss**: Mean Squared Error (MSE)

### **Anomaly Detection**
- **Threshold**: 99.5th percentile of reconstruction error on normal data
- **Detection**: MSE > Threshold â†’ Anomaly
- **Window Size**: 60 samples
- **Stride**: 10 samples (training), 1 sample (inference)

---

## ğŸ”§ Configuration

### **Kafka Configuration** (`config/kafka_config.yaml`)
```yaml
bootstrap_servers: localhost:9092
topics:
  sensor_data: swat-sensor-data
  anomalies: swat-anomalies
  alerts: swat-alerts
consumer_group: swat-consumer-group
```

### **Spark Configuration** (`config/spark_config.yaml`)
```yaml
app_name: SWaT-Anomaly-Detection
master: local[*]
window_duration: 60s
watermark_delay: 10s
```

### **Model Configuration** (`config/model_config.yaml`)
```yaml
window_size: 60
stride: 10
epochs: 10
batch_size: 64
threshold_percentile: 99.5
```

---

## ğŸ“ˆ Performance Metrics

- **Throughput**: ~1000 events/second (Kafka + Spark)
- **Latency**: <100ms (inference time)
- **Accuracy**: 95%+ on SWaT dataset
- **False Positive Rate**: <2%

---

## ğŸš¨ Alerting System

### **Alert Levels**
1. **INFO**: Minor deviations (MSE 1-1.5x threshold)
2. **WARNING**: Moderate anomalies (MSE 1.5-2x threshold)
3. **CRITICAL**: Severe anomalies (MSE >2x threshold)

### **Alert Channels**
- Console logging
- Dashboard notifications
- Email alerts (optional)
- SMS alerts (optional)
- Webhook integrations

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test
pytest tests/test_inference.py
```

---

## ğŸ“ Logging

Logs are stored in `logs/` directory with rotation:
- `app.log` - Application logs
- `kafka.log` - Kafka producer/consumer logs
- `spark.log` - Spark streaming logs
- `inference.log` - Model inference logs

---

## ğŸ³ Docker Deployment

### **Build and Run**
```bash
docker-compose up -d
```

### **Services**
- `zookeeper` - Kafka coordination
- `kafka` - Message broker
- `spark-master` - Spark master node
- `spark-worker` - Spark worker nodes
- `fastapi` - REST API server
- `streamlit` - Dashboard
- `postgres` - Database (optional)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™ Acknowledgments

- **SWaT Dataset**: Singapore University of Technology and Design (SUTD)
- **TensorFlow Team**: Deep learning framework
- **Apache Foundation**: Kafka and Spark
- **FastAPI Team**: Modern web framework

---

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ for Industrial Cybersecurity**
