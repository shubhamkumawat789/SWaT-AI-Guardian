# ğŸ›¡ï¸ Secure Water Treatment (SWaT) Anomaly Detection System

## ğŸ¯ Project Overview

An advanced real-time anomaly detection system for Secure Water Treatment (SWaT) infrastructure using deep learning, streaming analytics, and distributed computing.

## ğŸ§° Tech Stack

### **Programming & Data Processing**
- **Python 3.9+** - Core programming language
- **NumPy** - Numerical computing
- **Pandas** - Data manipulation and analysis

### **Machine Learning & Deep Learning**
- **TensorFlow/Keras** - Autoencoder model for anomaly detection
- **Scikit-learn** - Preprocessing, scaling, Isolation Forest (optional ensemble)

### **Streaming & Big Data**
- **Apache Kafka** - Real-time data streaming and message queuing
- **Apache Spark** - Distributed data processing
- **Spark Structured Streaming** - Stream processing with event-time handling
- **PySpark SQL** - Feature engineering and window functions

### **Backend & API**
- **FastAPI** - High-performance REST API
- **Uvicorn** - ASGI server for FastAPI

### **Data Ingestion**
- **Kafka-Python** - Python client for Kafka

### **Visualization & Dashboard**
- **Streamlit** - Interactive web dashboard
- **Altair** - Declarative statistical visualization
- **Matplotlib** - Static plotting
- **Seaborn** - Statistical data visualization
- **Plotly** - Interactive charts

### **Dataset**
- **SWaT (Secure Water Treatment)** - Industrial control system dataset

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SWaT Sensors   â”‚
â”‚  (CSV Simulation)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer  â”‚ â—„â”€â”€ Streams sensor data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topics   â”‚
â”‚  - sensor-data  â”‚
â”‚  - anomalies    â”‚
â”‚  - alerts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Consumer  â”‚
â”‚   + Spark       â”‚ â—„â”€â”€ Window-based feature engineering
â”‚  Streaming      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Inference Engine       â”‚
â”‚  - Autoencoder (TensorFlow) â”‚
â”‚  - Isolation Forest (sklearn)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI     â”‚ â”‚ Alerting â”‚ â”‚  Streamlit  â”‚
â”‚  REST API    â”‚ â”‚  System  â”‚ â”‚  Dashboard  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
secure-water-treatment/
â”œâ”€â”€ .agent/
â”‚   â””â”€â”€ workflows/          # Development workflows
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ kafka_config.yaml   # Kafka configuration
â”‚   â”œâ”€â”€ spark_config.yaml   # Spark settings
â”‚   â””â”€â”€ model_config.yaml   # Model hyperparameters
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original SWaT dataset
â”‚   â”œâ”€â”€ processed/          # Preprocessed data
â”‚   â””â”€â”€ streaming/          # Streaming data buffer
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ autoencoder/        # Trained autoencoder models
â”‚   â”‚   â”œâ”€â”€ model.h5
â”‚   â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â”‚   â””â”€â”€ threshold.json
â”‚   â””â”€â”€ isolation_forest/   # Isolation Forest models
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_ingestion/
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py    # Stream data to Kafka
â”‚   â”‚   â””â”€â”€ kafka_consumer.py    # Consume from Kafka
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ spark_preprocessing.py  # Spark-based preprocessing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train_autoencoder.py    # Train deep learning model
â”‚   â”‚   â””â”€â”€ train_isolation_forest.py  # Train ensemble model
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ inference_engine.py     # Core inference logic
â”‚   â”‚   â””â”€â”€ streaming_inference.py  # Real-time inference
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ fastapi_server.py       # REST API endpoints
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ streamlit_app.py        # Interactive dashboard
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb  # Data exploration
â”œâ”€â”€ tests/                  # Unit and integration tests
â”œâ”€â”€ logs/                   # Application logs
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ docker-compose.yml      # Multi-container orchestration
â”œâ”€â”€ Dockerfile              # Container definition
â””â”€â”€ README.md              # This file
```

---

## âš¡ Deployment Guide
This system is designed for **One-Click Deployment**.

### Prerequisites
*   Docker & Docker Desktop (with WSL 2 Backend)
*   NVIDIA GPU (Optional, CPU fallback supported)

### Quick Start
1.  **Clone the Repository**
    ```bash
    git clone <repo-url>
    cd secure-water-treatment-system
    ```

2.  **Launch the System**
    Run the production startup script:
    ```bash
    bash start_system.sh
    ```
    *This script handles container orchestration, network creation, and health checks.*

3.  **Access the Interface**
    *   **Dashboard**: [http://localhost:8501](http://localhost:8501)
    *   **API Docs**: [http://localhost:8000/docs](http://localhost:8000/docs)
    *   **Kafka Manager**: [http://localhost:8081](http://localhost:8081)

---

## ğŸ“Š Model Details

### **Autoencoder Architecture**
- **Input Layer**: Window features (mean, std, min, max, slope)
- **Encoder**: Dense layers with ReLU activation
- **Bottleneck**: Compressed representation
- **Decoder**: Reconstruction layers
- **Output**: Reconstructed features
- **Loss**: Mean Squared Error (MSE)

### **Anomaly Detection**
- **Threshold**: 99.5th percentile of reconstruction error on normal data
- **Detection**: MSE > Threshold â†’ Anomaly
- **Window Size**: 60 samples
- **Stride**: 10 samples (training), 1 sample (inference)

---

## ğŸ”§ Configuration

### **Kafka Configuration** (`config/kafka_config.yaml`)
```yaml
bootstrap_servers: localhost:9092
topics:
  sensor_data: swat-sensor-data
  anomalies: swat-anomalies
  alerts: swat-alerts
consumer_group: swat-consumer-group
```

### **Spark Configuration** (`config/spark_config.yaml`)
```yaml
app_name: SWaT-Anomaly-Detection
master: local[*]
window_duration: 60s
watermark_delay: 10s
```

### **Model Configuration** (`config/model_config.yaml`)
```yaml
window_size: 60
stride: 10
epochs: 10
batch_size: 64
threshold_percentile: 99.5
```

---

## ğŸ“ˆ Performance Metrics

- **Throughput**: ~1000 events/second (Kafka + Spark)
- **Latency**: <100ms (inference time)
- **Accuracy**: 95%+ on SWaT dataset
- **False Positive Rate**: <2%

---

## ğŸš¨ Alerting System

### **Alert Levels**
1. **INFO**: Minor deviations (MSE 1-1.5x threshold)
2. **WARNING**: Moderate anomalies (MSE 1.5-2x threshold)
3. **CRITICAL**: Severe anomalies (MSE >2x threshold)

### **Alert Channels**
- Console logging
- Dashboard notifications
- Email alerts (optional)
- SMS alerts (optional)
- Webhook integrations

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest --cov=src tests/

# Run specific test
pytest tests/test_inference.py
```

---

## ğŸ“ Logging

Logs are stored in `logs/` directory with rotation:
- `app.log` - Application logs
- `kafka.log` - Kafka producer/consumer logs
- `spark.log` - Spark streaming logs
- `inference.log` - Model inference logs

---

## ğŸ³ Docker Deployment

### **Build and Run**
```bash
docker-compose up -d
```

### **Services**
- `zookeeper` - Kafka coordination
- `kafka` - Message broker
- `spark-master` - Spark master node
- `spark-worker` - Spark worker nodes
- `fastapi` - REST API server
- `streamlit` - Dashboard
- `postgres` - Database (optional)

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ™ Acknowledgments

- **SWaT Dataset**: Singapore University of Technology and Design (SUTD)
- **TensorFlow Team**: Deep learning framework
- **Apache Foundation**: Kafka and Spark
- **FastAPI Team**: Modern web framework

---

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

---

**Built with â¤ï¸ for Industrial Cybersecurity**

